{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../idp.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"background-color:#2F5597;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">Aula 5 - Risco de fraude de crédito</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Introdução </h2>\n",
    "Neste projeto vamos aplicar algoritmos de inteligência artificial para a detecção de fraudes em cartões de crédito, um problema muito grande que as instituições financeiras e Fintechs têm enfrentado diariamente, que é identificar se uma transação é uma fraude ou não. É de extrema importância que as operadoras de cartão de crédito estejam preparadas para esse tipo de crime, monitorando constantemente o comportamento dos cartões.\n",
    "\n",
    "<h2> Objetivos desse projeto </h2>\n",
    "O objetivo é fazer uso do aprendizado de máquina para identificar e classificar as fraudes, reduzindo posteriormente o número de fraudes que acabam ocorrendo e que não são identificadas a tempo. Para resolver esse tipo de problema, vamos trabalhar com o que conhecemos como classes desbalanceadas. Utilizaremos técnicas de oversampling e undersampling para mitiga esses problemas. \n",
    "\n",
    "Mais informações: https://imbalanced-learn.org/stable/auto_examples/index.html#general-examples\n",
    "\n",
    "<h2> Dados</h2>\n",
    "Os conjuntos de dados contêm transações realizadas com cartões de crédito em setembro de 2013 por titulares de cartões europeus. Esse conjunto de dados apresenta transações que ocorreram em 2 dias, nas quais temos 492 fraudes em 284.807 transações. O conjunto de dados é altamente desequilibrado, a classe positiva (fraude) representa 0,172% de todas as transações.\n",
    "\n",
    "O cojunto de dados contém apenas variáveis numéricas que são o resultado de uma transformação de PCA. Infelizmente, devido a questões de confidencialidade , não são fornecidos os detalhes dos atributos e suas características.\n",
    "\n",
    "Atributos V1, V2,… V28 são os principais componentes obtidos com o PCA, as únicas características que não foram transformadas com o PCA são 'Tempo' e 'Valor' (Valor da transação).\n",
    "\n",
    "- O atributo 'Time' contém os segundos entre cada transação e a primeira transação no conjunto de dados.\n",
    "\n",
    "- O atributo 'Amount' é o valor da transação.\n",
    "\n",
    "- O atributo 'Class' é a variável de resposta e assume o valor 1 em caso de fraude e 0 em caso de não fraude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUbMo63Y6jjC"
   },
   "source": [
    "<img src=\"creditcard.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fzWxNPh7Kfb",
    "outputId": "122d8e68-e863-425f-b74b-150ebf6eab6b"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn.metrics import classification_report, recall_score, precision_score ,average_precision_score, plot_precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from yellowbrick.classifier import PrecisionRecallCurve, ConfusionMatrix\n",
    "from sklearn.model_selection import train_test_split, cross_validate ,KFold, StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PowerTransformer, QuantileTransformer, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier \n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "%matplotlib inline \n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rsfF0bV7Opm",
    "outputId": "16983e3c-a93e-44f1-8290-82d94d232ef6"
   },
   "outputs": [],
   "source": [
    "# Importando os dados\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, todas as colunas estão já normalizadas, em escala e anonimizadas. Assim não conseguimos saber o que significa cada atributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LE-i8dIu7QdW",
    "outputId": "50c71f61-00c1-435e-8636-4532824a6ced"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos um total de 31 colunas e 284 mil registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHg6onso8JZr",
    "outputId": "7ccc4fe0-f2b9-4c8e-f882-e466cc84dbef"
   },
   "outputs": [],
   "source": [
    "# Análise estatística básica\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jD8df05B8J0X",
    "outputId": "6c35d959-7abc-4091-95a2-91d981298173"
   },
   "outputs": [],
   "source": [
    "# Verificando a distribuição das classes 0 e 1\n",
    "sns.countplot(x=data['Class'], palette='Pastel2')\n",
    "print('Normal: {} |  Fraud: {}'.format(data[data['Class']==0].shape[0] , data[data['Class']==1].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As classes são muito distorcidas, precisamos resolver esse problema. E isso é o principal objetivo desse notebook da Aula 5.\n",
    "print('Não Fraudes:', round(data['Class'].value_counts()[0]/len(data) * 100,2), '% do dataset')\n",
    "print('Fraudes:', round(data['Class'].value_counts()[1]/len(data) * 100,2), '% do dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:**  Observe como nosso conjunto de dados original está desequilibrado! A maioria das transações não são fraudes. Se usarmos esse dataframe como base para nossos modelos e análises preditivas, podemos obter muitos erros e nossos algoritmos provavelmente se ajustarão, pois \"suporão\" que a maioria das transações não é fraude. Mas não queremos que nosso modelo assuma, queremos que nosso modelo detecte padrões que dão sinais de fraude!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao ver as distribuições, podemos ter uma ideia de quão distorcidos estão as variáveis, também podemos ver outras distribuições de outras variáveis. Existem técnicas que podem ajudar as distribuições a serem menos distorcidas e que serão implementadas neste notebook. Vamos fazer engenharia de atributos com esses dados, a fim de maximizar as métricas : <b> ROC AUC | Precision | Recall </b>.\n",
    "\n",
    "Tentar medir o desempenho do modelo com Acurácia seria um grande erro, pois teríamos uma alta acurácia, o que de fato não resolveria nosso problema, pois o conjunto possui classes desbalanceadas. Assim, vamos focar nessas três métricas listadas acima, que não variam com o desequilíbrio das classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise dos Dados\n",
    "\n",
    "Primeiro iremos verificar a distribuição de segundos, tentando identificar o período de tempo em que uma transação fraudulenta é feita, em comparação com transações normais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos entender a coluna Time primeiro.\n",
    "print(\"Tempo inicial em segundos:\", data['Time'].min())\n",
    "print(\"Tempo final (em segundos)\", data['Time'].max())\n",
    "print('Tempo em dias:', 172792/86400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18,4))\n",
    "\n",
    "amount_val = data['Amount'].values\n",
    "time_val = data['Time'].values\n",
    "\n",
    "sns.distplot(amount_val, ax=ax[0], color='b')\n",
    "ax[0].set_title('Distribuição do Valor da Transação', fontsize=14)\n",
    "ax[0].set_xlim([min(amount_val), max(amount_val)])\n",
    "\n",
    "sns.distplot(time_val, ax=ax[1], color='g')\n",
    "ax[1].set_title('Distribuição do Tempo das Transações (em segundos)', fontsize=14)\n",
    "ax[1].set_xlim([min(time_val), max(time_val)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.title('Temporariedade das transações não-fraudes')\n",
    "sns.distplot(data[data['Class']==0]['Time'], color='green')\n",
    "\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('Temporariedade das transações fraudes')\n",
    "sns.distplot(data[data['Class']==1]['Time'], color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1wuilOF8rDi"
   },
   "source": [
    "Observando as duas distribuições, podemos ver que o período de uma transação rotulada como Normal, ocorre em média de forma trivial o que é considerado algo normal. Já as transações fraudulentas ocorrem com menor frequência (obviamente) e com uma lacuna bem importante nos tempos entre 100k a 130k. \n",
    "\n",
    "Precisamos verificar posteriormente, se uma transação fraudulenta ocorre mais de uma vez, com os mesmos dados da transação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbzozsr-8oen",
    "outputId": "197abbba-0699-411a-84b1-fac06c278168"
   },
   "outputs": [],
   "source": [
    "# Existem transações repetidas?\n",
    "\n",
    "# Existe um método duplicated() no pandas\n",
    "# Link: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html\n",
    "\n",
    "fraude = data[data['Class']==1].loc[data.duplicated(keep=False)]\n",
    "print('Transações repetidas: {} '.format(len(fraude)))\n",
    "print('\\n')\n",
    "fraude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7P2rWKn8w9K"
   },
   "source": [
    "<br>\n",
    "Foram <b> 32 transações repetidas </b> nestes dois dias, há casos em que ocorreram três fraudes com o mesmo valor. Algo que chama a atenção são os baixos valores das transações que houve um número maior de fraudes, talvez porque o fraudador considere o risco de ser pego em relação ao número de tentativas.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9dYdnPu8urk"
   },
   "outputs": [],
   "source": [
    "# cmap\n",
    "cmap = sns.diverging_palette(120, 40, sep=20, as_cmap=True, center='dark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIv289qb8zhk",
    "outputId": "2b823754-a201-4c40-8644-c9156f64da97"
   },
   "outputs": [],
   "source": [
    "# Correlação\n",
    "corr = data.corr(method='pearson')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(23,15))\n",
    "\n",
    "# cmap=Greys\n",
    "\n",
    "plt.title('Matriz de correlação', fontsize=16)\n",
    "print('\\n')\n",
    "correlacao = sns.heatmap(corr, annot=True, cmap='Blues', ax=ax, lw=3.3, linecolor='lightgray')\n",
    "correlacao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HYsFrqx834t"
   },
   "source": [
    "Observando as correlações de Pearson, podemos ver que não há grandes correlações positivas, apenas algumas características ultrapassam <b> 0,30 de correlação, que são <b> V7 com Amount e V20 com Amount </b>. Temos também correlações negativas interessantes, V2 com Amount (-0,53) e V5 com Amount (-0,39).\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHvQfNsz9iEz"
   },
   "source": [
    "#### Distribuições\n",
    "\n",
    "Vamos explorar um pouco as distribuições, das variáveis mascaradas <b> (V) </b>, para ver como elas se comportam, nesses dois dias de transações financeiras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gc14OE0i81Kd",
    "outputId": "bbf3836f-a526-495f-f821-3f600c6c80d5"
   },
   "outputs": [],
   "source": [
    "# \n",
    "cols_names = data.drop(['Class', 'Amount', 'Time'], axis=1)\n",
    "idx = 0\n",
    "\n",
    "# Separando as classes 0 e 1\n",
    "fraud = data[data['Class']==1]\n",
    "normal = data[data['Class']==0]\n",
    "\n",
    "# Plotando a figura  \n",
    "fig, ax = plt.subplots(nrows=7, ncols=4, figsize=(18,18))\n",
    "fig.subplots_adjust(hspace=1, wspace=1)\n",
    "\n",
    "for col in cols_names:\n",
    "    idx += 1\n",
    "    plt.subplot(7, 4, idx)\n",
    "    sns.kdeplot(fraud[col], label=\"Normal\", color='green', shade=True)\n",
    "    sns.kdeplot(normal[col], label=\"Fraud\", color='red', shade=True)\n",
    "    plt.title(col, fontsize=11)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6s_Gkdfa9tx4"
   },
   "source": [
    "Todas as distribuições de variáveis que possuem uma \"máscara\", não sabemos a real representação dessas variáveis porque elas estão ocultas, mas através da distribuição com a densidade da informação, dá para ver muito bem as curvas de cada uma, comparando com uma normal transação ou fraude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ONJVBH9-CXp",
    "outputId": "06f80836-af23-4723-b11e-5170c581f06f"
   },
   "outputs": [],
   "source": [
    "# Qual o valor médio das transações fraudulentas\n",
    "print('Média de valor de fraude: {} | Média de valor normal: {}'.format(fraud['Amount'].mean() , normal['Amount'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos validar essas informações\n",
    "fraud['Amount'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal['Amount'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JPIJe97-QBm",
    "outputId": "32b8fb5f-3bb9-4de8-e769-aeac97715ba6"
   },
   "outputs": [],
   "source": [
    "# Qual o maior valor de fraude? e do valor normal?\n",
    "print('O valor mais alto de fraude é: {}  | Maior valor de transação normal: {}'.format(fraud['Amount'].max(), normal['Amount'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_6gmCmX-TMW",
    "outputId": "35724570-2cc4-4d33-ba85-809b3408657e"
   },
   "outputs": [],
   "source": [
    "# Distribuição da quantidade de transações\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('Transações', fontsize=14)\n",
    "plt.grid(False)\n",
    "sns.kdeplot(data['Amount'], color='lightblue', shade=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos fazer a análise exploratória utilizando o Dataprep\n",
    "from dataprep.eda import plot, create_report\n",
    "\n",
    "# Utilizando o método plot para construção do relatório EDA de forma automática dos nomes dos restaurantes\n",
    "plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41_pBNHq-1Vs"
   },
   "source": [
    "### Análise dos dados\n",
    "\n",
    "Após essa etapa de exploração dos dados, conseguimos ter bons insights sobre o todo, transações fraudulentas tendem a ter valores maiores que transações normais, fraudes repetidas contém valores menores.\n",
    "\n",
    "Iremos criar vários experimentos, em busca de um modelo robusto que atinja nosso objetivo.\n",
    "\n",
    "<br>\n",
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXuIva5A-4Tf"
   },
   "source": [
    "## Modelo base (sem balanceamento) - Regressão Logística\n",
    "\n",
    "Vamos criar um modelo base, para poder comparar os próximos resultados de outros experimentos, com esse modelo puro e simples que vamos construir.\n",
    "\n",
    "Ou seja, nesse modelo não será aplicada nenhuma técnica de balanceamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jOKUSgT-1tP"
   },
   "outputs": [],
   "source": [
    "# Dividindo as classes em fraude e não-fraude\n",
    "\n",
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "# Treino / Teste (70/30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n",
    "\n",
    "# StandardScaler \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Encoder \n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-rgHfe6-_pB",
    "outputId": "27d66b3c-641f-4638-e901-3615ddaf92de"
   },
   "outputs": [],
   "source": [
    "# Modelo base\n",
    "\n",
    "baseline = LogisticRegression(random_state=42)\n",
    "baseline.fit(X_train, y_train)\n",
    "y_baseline = baseline.predict(X_test)\n",
    "\n",
    "# probabilidades\n",
    "y_proba_baseline = baseline.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_baseline))\n",
    "print('\\n')\n",
    "print('AUC: {}%'.format(roc_auc_score(y_test, y_proba_baseline)))\n",
    "print('Precision-Recall: {}'.format(average_precision_score(y_test, y_proba_baseline)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qulWsrgI_Dsn"
   },
   "source": [
    "### Resultados do modelo base\n",
    "\n",
    "* AUC: 0.97\n",
    "* AUPRC: 0.78\n",
    "* Precision: 0.88\n",
    "* Recall: 0.63 (valor extremamente baixo)\n",
    "\n",
    "Os resultados nos dão uma ideia de quanto podemos melhorar o modelo de base, para um modelo melhor. Vamos tentar executar outros experimentos para obter um melhor desempenho.\n",
    "Lembrando que o valor do AUC varia de 0,0 até 1,0 e o limiar entre a classe é 0,5. Ou seja, acima desse limite, o algoritmo classifica em uma classe e abaixo na outra classe.\n",
    "\n",
    "Podemos ver como o recall teve um valor extremamente baixo. Isso significa que ele não acerta muito bem a classe 1 (fraude).\n",
    "\n",
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dessa forma, vamos rodar 3 experimentos:\n",
    "\n",
    "- **Experimento 1**: Random Forest + Oversampling (Smote)\n",
    "- **Experimento 2**: XGBoost + Oversampling (Smote)\n",
    "- **Experimento 3**: XGBoost + Undersampling (Nearmiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnV73Zli_G2r"
   },
   "source": [
    "### Experimento 1: Random Forest + Oversampling (Smote)\n",
    "\n",
    "Vamos aplicar técnicas de balanceamento de dados, e também validar nossos modelos a partir da validação cruzada. Vamos primeiro aplicar um Random Forest, combinado com uma técnica de OverSampling que basicamente irá, criar dados sintéticos na classe minoritária que é a classe de fraude, vamos testar essa abordagem e ver o desempenho do modelo. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_e0aCxxC_J1r"
   },
   "source": [
    "\n",
    "<p align=center>\n",
    "<img src=\"https://blog.strands.com/hs-fs/hubfs/Screenshot%202019-07-18%20at%2014.15.15.png?width=600&name=Screenshot%202019-07-18%20at%2014.15.15.png\" width=\"60%\"></p>\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXlLJLcS_PAu"
   },
   "source": [
    "Vamos então construir um modelo Random Forest com 200 árvores combinado com a técnica OverSampling <b> SMOTE </b> e ver os resultados das métricas que queremos otimizar, que são:\n",
    "\n",
    "* AUC \n",
    "* Precision\n",
    "* Recall \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxZ4VBxE_Bz4",
    "outputId": "22b21ce2-07d4-40d3-8f75-24683f7d48ba"
   },
   "outputs": [],
   "source": [
    "# Vamos fazer o balanceamento dentro da validaçõ cruzada\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "# Validação cruzada \n",
    "KFold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in KFold.split(X,y):\n",
    "      fold += 1 \n",
    "      print('Fold: ', fold)\n",
    "      print('Train: ',train_index.shape[0])\n",
    "      print('Test: ', test_index[0])\n",
    "\n",
    "      # Separando as classes\n",
    "      X = data.drop('Class', axis=1)\n",
    "      y = data['Class']\n",
    "\n",
    "      # OverSampling SMOTE\n",
    "      smote = SMOTE(random_state=42)\n",
    "      X, y = smote.fit_resample(X, y)\n",
    "      print('Normal: {}  |  Fraud: {}'.format(np.bincount(y)[0], np.bincount(y)[1])) #Contar o número de ocorrências de cada valor na matriz de inteiros não negativos.\n",
    "\n",
    "      # Dividindo os dados  \n",
    "      X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "      y_train, y_test = y[train_index], y[test_index] \n",
    "\n",
    "      # Pré-processamento\n",
    "      scaler = QuantileTransformer(random_state=42)\n",
    "      X_train = scaler.fit_transform(X_train)\n",
    "      X_test = scaler.transform(X_test)\n",
    "\n",
    "      # Construindo o modelo \n",
    "      forest = RandomForestClassifier(n_estimators=200, max_depth=13, min_samples_split=9,\n",
    "                                    random_state=42)\n",
    "      forest.fit(X_train, y_train)\n",
    "      y_pred_forest = forest.predict(X_test)\n",
    "      y_proba_forest = forest.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "      # Métricas \n",
    "      print('\\n')\n",
    "      print(classification_report(y_test, y_pred_forest))\n",
    "      print('--------------'*5)\n",
    "      print('\\n')\n",
    "      auc_forest = roc_auc_score(y_test, y_proba_forest)\n",
    "      precision_forest = precision_score(y_test, y_pred_forest)\n",
    "      recall_forest = recall_score(y_test, y_pred_forest)\n",
    "      auprc_forest = average_precision_score(y_test, y_proba_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zooVcS5_RzS",
    "outputId": "e3330dcf-4aa0-4655-9acb-9fc5d7b6375e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Métricas do Experimento 1\n",
    "print('Random Forest')\n",
    "print('\\n')\n",
    "\n",
    "print('AUC: ', np.mean(auc_forest))\n",
    "print('Precision: ', np.mean(precision_forest))\n",
    "print('Recall: ', np.mean(recall_forest))\n",
    "print('Precision-Recall: ', np.mean(auprc_forest))\n",
    "\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "\n",
    "# Curva ROC do Random Forest \n",
    "auc_forest = np.mean(auc_forest)\n",
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_test, y_proba_forest)\n",
    "\n",
    "# Plotando o gráfico \n",
    "plt.figure(figsize=(12,7))\n",
    "plt.plot(fpr_forest, tpr_forest, color='blue', label='AUC: {}'.format(auc_forest))\n",
    "plt.fill_between(fpr_forest, tpr_forest, color='skyblue', alpha=0.3)\n",
    "plt.plot([0,1], [0,1], color='black', ls='--', label='Reference line')\n",
    "plt.xlabel('False Positive Rate', fontsize=14)\n",
    "plt.ylabel('True Positive Rate', fontsize=14)\n",
    "plt.title('ROC Random Forest', fontsize=16)\n",
    "plt.legend(loc=4, fontsize=14)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N09gRQua_eFn"
   },
   "source": [
    "\n",
    "### Resultados do Experimento 1\n",
    "\n",
    "AUC:  0.972695203329736  \n",
    "Precision:  0.927710843373494  \n",
    "Recall:  0.7857142857142857  \n",
    "Precision-Recall:  0.8257579754337053  \n",
    "\n",
    "Em resumo, o modelo obteve resultados satisfatórios. A AUC do modelo foi alta, na validação o resultado foi <b> 97% </b>, porém as demais métricas não atingiram um valor tão bom de melhoria quanto se esperava ao aplicar a técnica de balanceamento. A precisão foi alta, mas o Recall não aumentou muito em comparação com a modelo base. O Precision-Recall foi de 0.82 (contra 0.78 no modelo base) e é a principal métrica em nossa avaliação e é difícil de otimizar, porque queremos um modelo que possa separar bem a fraude de uma transação normal, e também para identificar bem uma fraude quando ela realmente ocorre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-whL7Jl_zWk"
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGEhwQjM_2-4"
   },
   "source": [
    "### Experimento 2: XGBoost + Oversampling (Smote)\n",
    "\n",
    "XGBoost é um algoritmo de Aprendizado de Máquina, baseado em uma árvore de decisão e que usa uma estrutura de Gradient Boosting\n",
    "\n",
    "Na previsão de problemas envolvendo dados não estruturados, como imagens, textos e vídeos, as redes neurais artificiais tendem a superar todos os outros algoritmos ou frameworks. No entanto, quando se trata de dados estruturados / tabulares, os algoritmos baseados em árvore de decisão são considerados os melhores em sua classe no momento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8RZVYq6_57h"
   },
   "source": [
    "Vamos agora aplicar a técnica OverSampling com o XGboost e para isso vamos usar a técnica de <b>SMOTE</b> que será responsável pela aplicação do OverSampling, igualando as classes maximizando as proporções da classe 1.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82192Lk1_uQj",
    "outputId": "650d6376-5e08-42cd-9efd-cd955d9fae10"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "# Validação\n",
    "KFold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    " \n",
    "precision_xgboost = []\n",
    "recall_xgboost = []\n",
    "auc_xgboost = []\n",
    "precision_recall_xgboost = []\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in KFold.split(X,y):\n",
    "      fold += 1 \n",
    "      print('Fold: ', fold)\n",
    "      print('Train: ',train_index.shape[0])\n",
    "      print('Test: ', test_index[0])\n",
    "    \n",
    "      # OverSampling com SMOTE \n",
    "      smt = SMOTE(random_state=42)\n",
    "      X, y = smt.fit_resample(X, y)\n",
    "      print('Normal: {}  |  Fraud: {}'.format(np.bincount(y)[0], np.bincount(y)[1]))\n",
    "     \n",
    "\n",
    "      # Dividindo o dataset \n",
    "      X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "      y_train, y_test = y[train_index], y[test_index] \n",
    "\n",
    "      \n",
    "      # Pré-processamento\n",
    "      scaler = QuantileTransformer()\n",
    "      X_train = scaler.fit_transform(X_train)\n",
    "      X_test = scaler.transform(X_test)\n",
    "\n",
    "      # XGboost \n",
    "      xgb = XGBClassifier(n_estimators=300, max_delta_step=1 ,eval_metric='aucpr', \n",
    "                          cpu_history='gpu', random_state=42)\n",
    "      xgb.fit(X_train, y_train)\n",
    "      y_pred = xgb.predict(X_test)\n",
    "  \n",
    "\n",
    "      # Métricas \n",
    "      precision_recall_xgboost = average_precision_score(y_test, y_pred)\n",
    "      precision_xgboost = precision_score(y_test, y_pred)\n",
    "      recall_xgboost = recall_score(y_test, y_pred)\n",
    "      auc_xgboost  = roc_auc_score(y_test, y_pred)\n",
    "      print('Precision-Recall: ', average_precision_score(y_test, y_pred))\n",
    "      print('\\n')\n",
    "      print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "# Validação Final\n",
    "print('Precision-Recall: ', np.mean(precision_recall_xgboost))\n",
    "print('Recall: ', np.mean(recall_xgboost))\n",
    "print('Precision: ', np.mean(precision_xgboost))\n",
    "print('AUC: ', np.mean(auc_xgboost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRDvRiDL_87J",
    "outputId": "f5a85664-ad30-4f0b-9ce2-8379c15f5ed8"
   },
   "outputs": [],
   "source": [
    "# Validação do XGboost + SMOTE\n",
    "print('XGboost')\n",
    "print('\\n')\n",
    "\n",
    "print('AUC: ', np.mean(auc_xgboost))\n",
    "print('Precision: ', np.mean(precision_xgboost))\n",
    "print('Recall: ', np.mean(recall_xgboost))\n",
    "print('Precision-Recall: ', np.mean(precision_recall_xgboost))\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "\n",
    "# Curva ROC para XGBoost\n",
    "roc_auc_xgboost = np.mean(auc_xgboost)\n",
    "fpr_xgboost, tpr_xgboost, thresholds_xgboost = roc_curve(y_test, y_pred)\n",
    "\n",
    "# Plotando a figura \n",
    "plt.figure(figsize=(12,7))\n",
    "plt.plot(fpr_xgboost, tpr_xgboost, color='blue', label='AUC: {}'.format(roc_auc_xgboost))\n",
    "plt.fill_between(fpr_xgboost, tpr_xgboost, color='skyblue', alpha=0.3)\n",
    "plt.plot([0,1], [0,1], color='black', ls='--', label='Reference line')\n",
    "plt.xlabel('False Positive Rate', fontsize=14)\n",
    "plt.ylabel('True Positive Rate', fontsize=14)\n",
    "plt.title('ROC XGboost', fontsize=16)\n",
    "plt.legend(loc=4, fontsize=14)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados do Experimento 2\n",
    "\n",
    "AUC:  0.8978712530331584  \n",
    "Precision:  0.8863636363636364  \n",
    "Recall:  0.7959183673469388  \n",
    "Precision-Recall:  0.705824215761466  \n",
    "\n",
    "Em resumo, o modelo 2 com XGBoost obteve resultados satisfatórios. A AUC do modelo foi alta, na validação o resultado foi <b> 89% </b>, porém as demais métricas não atingiram um valor tão bom de melhoria quanto se esperava ao aplicar a técnica de balanceamento. A precisão foi alta, mas o Recall não aumentou muito em comparação com a modelo base. O Precision-Recall foi de 0.70 (contra 0.78 no modelo base)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8I_AlJTpADWh"
   },
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "### Experimento 3: XGBoost + Undersampling (Nearmiss)\n",
    "\n",
    "\n",
    "Combinaremos o XGboost com a técnica Undersampling NearMiss. Usaremos os mesmos parâmetros do modelo anterior, neste caso minimizaremos a classe majoritária para uma proporção igual à nossa classe minoritária."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7sdZnXLABIC",
    "outputId": "eb4f19aa-0ade-4029-dd69-f288dc86da52"
   },
   "outputs": [],
   "source": [
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "# UnderSampling NearMiss\n",
    "under = NearMiss()\n",
    "X,y = under.fit_resample(X, y)\n",
    "\n",
    "# Validacão\n",
    "KFold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "resultados = []\n",
    "fold = 0\n",
    "for train_index, test_index in KFold.split(X,y):\n",
    "      fold += 1 \n",
    "      print('Fold: ', fold)\n",
    "      print('Train: ',train_index.shape[0])\n",
    "      print('Test: ', test_index[0])\n",
    "\n",
    "      # Classe desbalanceada\n",
    "      print('Normal: {} | Fraud: {}'.format(np.bincount(y)[0], np.bincount(y)[1]))\n",
    "\n",
    "\n",
    "      # Dividindo o dataset\n",
    "      X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "      y_train, y_test = y[train_index], y[test_index] \n",
    "\n",
    "      \n",
    "      # Pré-processamento\n",
    "      scaler = StandardScaler()\n",
    "      X_train = scaler.fit_transform(X_train)\n",
    "      X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "      # Encoder \n",
    "      encoder = LabelEncoder()\n",
    "      y_train = encoder.fit_transform(y_train)\n",
    "      y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "      # XGboost \n",
    "      xgb = XGBClassifier(n_estimators=300, max_delta_step=1 ,eval_metric='aucpr', \n",
    "                          cpu_history='gpu', random_state=42)\n",
    "      xgb.fit(X_train, y_train)\n",
    "      y_pred = xgb.predict(X_test)\n",
    "\n",
    "    # Métricas de avaliação\n",
    "      precision_recall_xgboost = average_precision_score(y_test, y_pred)\n",
    "      precision_xgboost = precision_score(y_test, y_pred)\n",
    "      recall_xgboost = recall_score(y_test, y_pred)\n",
    "      auc_xgboost  = roc_auc_score(y_test, y_pred)\n",
    "      print('Precision-Recall: ', average_precision_score(y_test, y_pred))\n",
    "      print('\\n')\n",
    "      print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "# Validação Final  \n",
    "print('Precision-Recall: ', np.mean(precision_recall_xgboost))\n",
    "print('Recall: ', np.mean(recall_xgboost))\n",
    "print('Precision: ', np.mean(precision_xgboost))\n",
    "print('AUC: ', np.mean(auc_xgboost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZR0lmGlAF-4",
    "outputId": "95c39dd1-0d3a-440a-db4d-982b38aa453c"
   },
   "outputs": [],
   "source": [
    "# Métricas do Experimento 3: XGboost + NearMiss  \n",
    "print('XGboost')\n",
    "print('\\n')\n",
    "\n",
    "print('AUC: ', np.mean(auc_xgboost))\n",
    "print('Precision: ', np.mean(precision_xgboost))\n",
    "print('Recall: ', np.mean(recall_xgboost))\n",
    "print('Precision-Recall: ', np.mean(precision_recall_xgboost))\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "\n",
    "# Curva ROC random forest \n",
    "roc_auc_xgboost = np.mean(auc_xgboost)\n",
    "fpr_xgboost, tpr_xgboost, thresholds_xgboost = roc_curve(y_test, y_pred)\n",
    "\n",
    "# Plotando a figura \n",
    "plt.figure(figsize=(12,7))\n",
    "plt.plot(fpr_xgboost, tpr_xgboost, color='blue', label='AUC: {}'.format(roc_auc_xgboost))\n",
    "plt.fill_between(fpr_xgboost, tpr_xgboost, color='skyblue', alpha=0.3)\n",
    "plt.plot([0,1], [0,1], color='black', ls='--', label='Reference line')\n",
    "plt.xlabel('False Positive Rate', fontsize=14)\n",
    "plt.ylabel('True Positive Rate', fontsize=14)\n",
    "plt.title('ROC XGboost', fontsize=16)\n",
    "plt.legend(loc=4, fontsize=14)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kP-dvWEhAQOd"
   },
   "source": [
    "### Resultados do Experimento 3\n",
    "\n",
    "AUC:  0.933673469387755  \n",
    "Precision:  0.956989247311828  \n",
    "Recall:  0.9081632653061225  \n",
    "Precision-Recall:  0.9150208470484968  \n",
    "\n",
    "A melhor combinação até agora foi o modelo 3 onde conseguimos obter valores muito bons para as principais métricas analizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyS8kYx65HNB",
    "outputId": "24811f0f-c024-4320-ccdf-e321a48911dc"
   },
   "outputs": [],
   "source": [
    "# Matriz de confusão\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Matriz de confusão XGboost', fontsize=15)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, cmap='Pastel2')\n",
    "\n",
    "print('\\n')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(xgb)\n",
    "shap_values = explainer(X)\n",
    "shap.summary_plot(shap_values,X, max_display=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPyy5hoZoJrG"
   },
   "source": [
    "\n",
    "\n",
    "Conseguimos visualizar nossa matriz de confusão, que mostra que o modelo conseguiu classificar bem as fraudes, minimizamos o Falso Negativo que contém Fraudes que foram classificadas incorretamente.\n",
    "\n",
    "Visualizando as variáveis mais importantes, que o modelo XGboost + NearMiss identificou, <b> V14 </b> é considerada pelo modelo como o atributo mais importante, é a de maior magnitude. Infelizmente, por estarem mascaradas, não conseguimos identificar do que se trata.\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JA7jgblOqNmB"
   },
   "source": [
    "### Conclusão\n",
    "\n",
    "Lidar com projetos onde as classes são desbalanceadas é sempre um desafio, e lidar com um problema tão delicado que é a fraude bancária, o maior desafio foi refletir sobre os insights que vimos sobre os dados, entender quais técnicas e algoritmos aplicar era de suma importância. É muito sensível lidar com transações que ocorrem em uma frequência tão alta e rápida. Dessa forma, resultado final foi satisfatório, onde conseguimos criar um modelo com um <b> Precision-Recall de: 98% </b> usando o <b> Combinação XGboost + NearMiss </b>, identificando uma fraude entre as transações, e que também conseguisse separar uma transação normal de uma fraude. \n",
    "\n",
    "<hr>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
